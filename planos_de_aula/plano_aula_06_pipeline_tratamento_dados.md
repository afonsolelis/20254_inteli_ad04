# Plano de Aula 06 - Pipeline e Tratamento de Dados

## üìã Informa√ß√µes Gerais

| **Propriedade** | **Valor** |
|-----------------|-----------|
| **Professor** | Afonso Cesar |
| **Unidade** | Semana 06 |
| **Data** | 18/11/2025 - 10:00h |
| **Dura√ß√£o Total** | 2 horas |
| **Modalidade** | Presencial |
| **Turma** | Computa√ß√£o - IoT |

## üéØ Objetivos de Aprendizagem

### Objetivos Gerais
- Desenvolver pipelines de dados para IoT
- Implementar processamento e transforma√ß√£o de dados
- Aplicar t√©cnicas de limpeza e valida√ß√£o no Projeto Atlas

### Objetivos Espec√≠ficos
Ao final desta aula, os alunos ser√£o capazes de:
- Entender conceitos de pipeline de dados
- Implementar ETL para dados IoT
- Aplicar t√©cnicas de limpeza e valida√ß√£o
- Desenvolver estrat√©gias de armazenamento
- Implementar processamento em tempo real e batch

## üìö Materiais e Recursos

### Materiais de Estudo Obrigat√≥rios (Pr√©-aula)
| **Material** | **Tipo** | **Dura√ß√£o** | **Objetivo** |
|---------------|----------|-------------|--------------|
| **Conceitos de Pipeline de Dados** | Artigo | 25 min | ETL, streaming, batch processing |
| **Ferramentas de Pipeline IoT** | Tutorial | 30 min | Apache Kafka, Apache Spark |
| **T√©cnicas de Limpeza de Dados** | Guia | 20 min | Valida√ß√£o, normaliza√ß√£o, filtragem |
| **Armazenamento de Dados IoT** | Whitepaper | 25 min | Bancos de dados, data lakes |

### Recursos da Aula
- Apache Kafka (confluent platform)
- Apache Spark (databricks community)
- InfluxDB (banco de dados temporal)
- Jupyter Notebooks
- Dados de exemplo IoT
- Computadores para programa√ß√£o

## ‚è∞ Cronograma Detalhado

### **Fase 1: Abertura e Contextualiza√ß√£o (15 min)**
- **10:00 - 10:15** | Daily inicial e revis√£o da coleta de dados

### **Fase 2: Conceitos de Pipeline (30 min)**
- **10:15 - 10:45** | ETL, streaming, batch processing

### **Fase 3: Laborat√≥rio Kafka (30 min)**
- **10:45 - 11:15** | Implementa√ß√£o de streaming de dados

### **Fase 4: Laborat√≥rio Spark (30 min)**
- **11:15 - 11:45** | Processamento e transforma√ß√£o de dados

### **Fase 5: Aplica√ß√£o LogiTrack (15 min)**
- **11:45 - 12:00** | Pipeline completo para o projeto

## üìñ Conte√∫do Program√°tico

### 1. **Contextualiza√ß√£o do Pipeline de Dados (15 min)**

#### 1.1 Import√¢ncia do Pipeline
- **Cen√°rio LogiTrack:** Milhares de dispositivos gerando dados
- **Volume:** Milh√µes de mensagens por dia
- **Variedade:** GPS, temperatura, vibra√ß√£o, status
- **Velocidade:** Tempo real vs. processamento batch

#### 1.2 Desafios do Pipeline IoT
- **Volume:** Big data em tempo real
- **Variedade:** Diferentes tipos de dados
- **Velocidade:** Processamento em tempo real
- **Qualidade:** Dados incompletos ou incorretos

### 2. **Conceitos de Pipeline (30 min)**

#### 2.1 ETL (Extract, Transform, Load)
1. **Extract (Extrair)**
   - **Fonte:** Dispositivos IoT, APIs, arquivos
   - **M√©todos:** Streaming, batch, real-time
   - **Aplica√ß√£o LogiTrack:** Dados de sensores

2. **Transform (Transformar)**
   - **Limpeza:** Remover dados inv√°lidos
   - **Normaliza√ß√£o:** Padronizar formatos
   - **Enriquecimento:** Adicionar contexto
   - **Aplica√ß√£o LogiTrack:** Processar dados GPS

3. **Load (Carregar)**
   - **Destino:** Bancos de dados, data lakes
   - **M√©todos:** Append, upsert, replace
   - **Aplica√ß√£o LogiTrack:** InfluxDB, PostgreSQL

#### 2.2 Processamento em Tempo Real vs Batch
1. **Streaming (Tempo Real)**
   - **Caracter√≠sticas:** Baixa lat√™ncia, alta throughput
   - **Ferramentas:** Apache Kafka, Apache Flink
   - **Aplica√ß√£o LogiTrack:** Alertas em tempo real

2. **Batch (Lote)**
   - **Caracter√≠sticas:** Alta lat√™ncia, processamento completo
   - **Ferramentas:** Apache Spark, Hadoop
   - **Aplica√ß√£o LogiTrack:** Relat√≥rios di√°rios

#### 2.3 T√©cnicas de Limpeza de Dados
1. **Valida√ß√£o**
   - **Range checking:** Valores dentro do esperado
   - **Format validation:** Formato correto dos dados
   - **Completeness:** Dados obrigat√≥rios presentes

2. **Normaliza√ß√£o**
   - **Unidades:** Padronizar unidades de medida
   - **Formato:** Padronizar formatos de data/hora
   - **Encoding:** Padronizar codifica√ß√£o de caracteres

3. **Filtragem**
   - **Outliers:** Remover valores extremos
   - **Duplicatas:** Remover dados duplicados
   - **Ru√≠do:** Filtrar ru√≠do dos sensores

### 3. **Laborat√≥rio Apache Kafka (30 min)**

#### 3.1 Configura√ß√£o do Kafka (10 min)
- **Objetivo:** Configurar cluster Kafka
- **Atividade:** Instalar e configurar Kafka
- **Comandos:**
```bash
# Download Kafka
wget https://downloads.apache.org/kafka/2.8.0/kafka_2.13-2.8.0.tgz
tar -xzf kafka_2.13-2.8.0.tgz

# Iniciar Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties

# Iniciar Kafka
bin/kafka-server-start.sh config/server.properties
```

#### 3.2 Producer (10 min)
- **Objetivo:** Implementar producer para enviar dados
- **Atividade:** Programar producer em Python
- **C√≥digo exemplo:**
```python
from kafka import KafkaProducer
import json
import time

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda x: json.dumps(x).encode('utf-8')
)

def send_sensor_data():
    data = {
        'timestamp': int(time.time()),
        'device_id': 'truck_001',
        'temperature': 25.5,
        'humidity': 60,
        'gps': {'lat': 40.7128, 'lon': -74.0060}
    }
    producer.send('logitrack-sensor-data', value=data)
    producer.flush()

while True:
    send_sensor_data()
    time.sleep(5)
```

#### 3.3 Consumer (10 min)
- **Objetivo:** Implementar consumer para receber dados
- **Atividade:** Programar consumer em Python
- **C√≥digo exemplo:**
```python
from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    'logitrack-sensor-data',
    bootstrap_servers=['localhost:9092'],
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

for message in consumer:
    data = message.value
    print(f"Received: {data}")
    
    # Processar dados
    process_sensor_data(data)
```

### 4. **Laborat√≥rio Apache Spark (30 min)**

#### 4.1 Configura√ß√£o do Spark (10 min)
- **Objetivo:** Configurar ambiente Spark
- **Atividade:** Instalar e configurar Spark
- **Comandos:**
```bash
# Download Spark
wget https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz
tar -xzf spark-3.2.0-bin-hadoop3.2.tgz

# Iniciar Spark
./bin/spark-shell
```

#### 4.2 Processamento de Dados (10 min)
- **Objetivo:** Implementar processamento com Spark
- **Atividade:** Programar transforma√ß√µes de dados
- **C√≥digo exemplo:**
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName("LogiTrackDataProcessing").getOrCreate()

# Ler dados do Kafka
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "logitrack-sensor-data") \
    .load()

# Processar dados
processed_df = df \
    .select(from_json(col("value").cast("string"), schema).alias("data")) \
    .select("data.*") \
    .filter(col("temperature") > 0) \
    .withColumn("timestamp", current_timestamp())

# Escrever para InfluxDB
query = processed_df \
    .writeStream \
    .outputMode("append") \
    .format("console") \
    .start()
```

#### 4.3 Armazenamento (10 min)
- **Objetivo:** Armazenar dados processados
- **Atividade:** Configurar InfluxDB
- **C√≥digo exemplo:**
```python
# Configurar InfluxDB
from influxdb import InfluxDBClient

client = InfluxDBClient(host='localhost', port=8086)
client.create_database('logitrack')

# Escrever dados
json_body = [{
    "measurement": "sensor_data",
    "tags": {
        "device_id": "truck_001"
    },
    "fields": {
        "temperature": 25.5,
        "humidity": 60
    },
    "time": "2023-01-01T00:00:00Z"
}]

client.write_points(json_body)
```

### 5. **Aplica√ß√£o LogiTrack (15 min)**

#### 5.1 Arquitetura do Pipeline (5 min)
- **Objetivo:** Definir arquitetura completa
- **Atividade:** Grupos desenham arquitetura
- **Componentes:** Kafka, Spark, InfluxDB, dashboards

#### 5.2 Implementa√ß√£o (5 min)
- **Objetivo:** Implementar pipeline completo
- **Atividade:** Grupos implementam componentes
- **Resultado:** Pipeline funcionando

#### 5.3 Monitoramento (5 min)
- **Objetivo:** Configurar monitoramento
- **Atividade:** Implementar m√©tricas e alertas
- **Resultado:** Sistema monitorado

## üé≠ Metodologia Pedag√≥gica

### **Laborat√≥rio Pr√°tico**
- **Hands-on:** Implementa√ß√£o real de pipelines
- **Aprendizagem ativa:** Descoberta atrav√©s da pr√°tica
- **Experimenta√ß√£o:** Testes com diferentes cen√°rios
- **Aplica√ß√£o:** Implementa√ß√£o em projeto real

### **Aprendizagem Baseada em Projetos**
- **Projeto real:** Pipeline para LogiTrack
- **Investiga√ß√£o:** An√°lise de requisitos e solu√ß√µes
- **Implementa√ß√£o:** Desenvolvimento de solu√ß√µes
- **Avalia√ß√£o:** Testes e valida√ß√£o

### **Discuss√£o Colaborativa**
- **Trabalho em equipe:** An√°lise conjunta de arquiteturas
- **Debate:** Compara√ß√£o de ferramentas
- **Consenso:** Decis√µes baseadas em evid√™ncias
- **Aplica√ß√£o:** Solu√ß√µes para o projeto

## üìä Avalia√ß√£o e Acompanhamento

### **Avalia√ß√£o Formativa**
- **Participa√ß√£o no laborat√≥rio:** 40%
- **Implementa√ß√£o t√©cnica:** 30%
- **Arquitetura do pipeline:** 30%

### **Crit√©rios de Avalia√ß√£o**
- **Compreens√£o t√©cnica:** Conhecimento de pipelines
- **Implementa√ß√£o pr√°tica:** Capacidade de programar
- **An√°lise cr√≠tica:** Arquitetura de solu√ß√µes
- **Colabora√ß√£o:** Trabalho em equipe eficaz

### **Feedback Imediato**
- **Durante o laborat√≥rio:** Orienta√ß√£o t√©cnica
- **Na implementa√ß√£o:** Coment√°rios sobre c√≥digo
- **Fechamento:** S√≠ntese dos aprendizados

## üöÄ Pr√≥ximos Passos

### **Para a Pr√≥xima Aula**
- **Tema:** Introdu√ß√£o ao Metabase
- **Prepara√ß√£o:** Pesquisar ferramentas de visualiza√ß√£o
- **Objetivo:** Criar dashboards para LogiTrack

### **Projeto Atlas**
- **Status:** Pipeline de dados implementado
- **Pr√≥ximo passo:** Cria√ß√£o de dashboards
- **Deliverables:** Pipeline funcionando com dados

## üìù Observa√ß√µes e Adapta√ß√µes

### **Poss√≠veis Adapta√ß√µes**
- **Sem ferramentas:** Usar simuladores e emuladores
- **Turma maior:** Rotacionar uso das ferramentas
- **Tempo reduzido:** Focar em implementa√ß√£o essencial

### **Recursos Alternativos**
- **Ambiente online:** Databricks Community, Confluent Cloud
- **Ferramentas limitadas:** Demonstra√ß√µes em grupo
- **Conex√£o limitada:** Trabalhar offline

### **Indicadores de Sucesso**
- **Engajamento:** Alunos participam ativamente do laborat√≥rio
- **Compreens√£o:** Conseguem implementar pipelines
- **Aplica√ß√£o:** Desenvolvem arquiteturas relevantes
- **Colabora√ß√£o:** Trabalham bem em equipe

## üìö Refer√™ncias e Materiais Complementares

### **Para Aprofundamento**
- Livro: "Building Real-Time Data Pipelines"
- Artigo: "Apache Kafka vs Apache Spark"
- Tutorial: "Data Processing with Apache Spark"

### **Para Pr√≥xima Aula**
- Preparar pesquisa sobre Metabase
- Revisar dados processados para visualiza√ß√£o
- Identificar requisitos de dashboards para LogiTrack

---

**Preparado por:** Afonso Cesar  
**Data de cria√ß√£o:** 18/11/2025  
**Vers√£o:** 1.0  
**Status:** Aprovado para execu√ß√£o
